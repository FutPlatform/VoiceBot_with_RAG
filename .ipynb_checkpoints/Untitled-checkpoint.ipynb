{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c737396e-a94c-4f5a-923b-fe281e06f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exisiting vector database\n",
      "Sound to text time: 1.3118021488189697\n",
      "يرجى تقديم الإجابة باللغة العربية فقط  واجعل الأجابة مختصرة في سطر قدر الامكان الامكان: \n",
      " السلام عليكم كيف الحال\n",
      "Final prompt to LLM after RAG:\n",
      "\n",
      "يرجى تقديم الإجابة باللغة العربية فقط  واجعل الأجابة مختصرة في سطر قدر الامكان الامكان: \n",
      " السلام عليكم كيف الحال\n",
      "Model response time: 0.851062536239624\n",
      "Text to sound time: 0.9589886665344238\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 165\u001b[0m\n\u001b[0;32m    157\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mzmlka\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSound Recordings\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRecording (3).m4a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m voice_bot \u001b[38;5;241m=\u001b[39m RAGVoiceBot(\n\u001b[0;32m    160\u001b[0m     knowldge_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./knowledge_base\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    161\u001b[0m     groq_token_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroq_token.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    162\u001b[0m     vector_db_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_db\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m )\n\u001b[1;32m--> 165\u001b[0m transcription, response \u001b[38;5;241m=\u001b[39m voice_bot\u001b[38;5;241m.\u001b[39mprocess_audio_file(audio_path)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSER:\u001b[39m\u001b[38;5;124m\"\u001b[39m, transcription)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from faster_whisper import WhisperModel\n",
    "from gtts import gTTS\n",
    "from groq import Groq\n",
    "from playsound import playsound \n",
    "import pyglet\n",
    "import io\n",
    "\n",
    "class RAGVoiceBot:\n",
    "    def __init__(self, vector_db_path, knowldge_path, groq_token_path, whisper_size='base', model_name='llama-3.1-70b-versatile'):\n",
    "\n",
    "        self.load_groq_token(groq_token_path)\n",
    "        self.grok_client = Groq()\n",
    "        \n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        self.whisper_model = self.initialize_STT_model(whisper_size)\n",
    "        self.llm = self.initialize_llm(model_name)\n",
    "        self.qa_pipeline = self.initialize_qa_pipeline(vector_db_path, knowldge_path, k=3)\n",
    "\n",
    "    def load_groq_token(self, groq_token_path):\n",
    "        with open(groq_token_path, 'r') as f:\n",
    "            os.environ[\"GROQ_API_KEY\"] = f.readline().strip()\n",
    "\n",
    "    def initialize_STT_model(self, whisper_size):\n",
    "        num_cores = os.cpu_count() // 2\n",
    "        return WhisperModel(\n",
    "            whisper_size,\n",
    "            device=\"cpu\",\n",
    "            # compute_type=\"int8\",\n",
    "            cpu_threads=num_cores\n",
    "        )\n",
    "        \n",
    "\n",
    "    def initialize_llm(self, model_name):\n",
    "        return ChatGroq(\n",
    "            model=model_name\n",
    "        )\n",
    "\n",
    "    def initialize_qa_pipeline(self, vector_db_path, knowldge_path, k):\n",
    "        if os.path.exists(vector_db_path):\n",
    "            print('Loading exisiting vector database')\n",
    "            vec_db = FAISS.load_local(vector_db_path, self.embedding_model, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print('Creating new vector database')\n",
    "            vec_db = self.create_vector_database_texts(knowldge_path)\n",
    "            vec_db.save_local(vector_db_path)\n",
    "\n",
    "        # The RAG pipeline\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=vec_db.as_retriever(search_kwargs={'k': 5}),\n",
    "            # return_source_documents=True\n",
    "        )\n",
    "\n",
    "    def create_vector_database_docs_pdfs(self, knowldge_path):\n",
    "        loader = PyPDFLoader(knowldge_path)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        return FAISS.from_documents(docs, self.embedding_model)\n",
    "    \n",
    "    def create_vector_database_texts(self, knowldge_path):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "        all_chunks = []\n",
    "        \n",
    "        for filename in os.listdir(knowldge_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(knowldge_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text_content = file.read()\n",
    "                    \n",
    "                    chunks = text_splitter.split_text(text_content)\n",
    "                    all_chunks.extend(chunks)\n",
    "                    \n",
    "        return FAISS.from_texts(all_chunks, self.embedding_model)\n",
    "    \n",
    "    def speach_to_text(self, wav_audio):\n",
    "        # print('we now opened the speech_to_text method .........')\n",
    "        # # segments, _ = self.whisper_model.transcribe(audio_path, language='ar')\n",
    "        # # return ''.join(segment.text for segment in segments)\n",
    "        # # with open(audio_path, \"rb\") as file:\n",
    "        # # audio_file \n",
    "        # audio_file = (\"audio.wav\", wav_audio)\n",
    "        # transcription = self.grok_client.audio.transcriptions.create(\n",
    "        #     file=audio_file, \n",
    "        #     model=\"whisper-large-v3-turbo\",\n",
    "        #     prompt=\"Specify context or spelling\",\n",
    "        #     response_format=\"json\",\n",
    "        #     # language='ar',\n",
    "        #     temperature=0.0,\n",
    "        # )\n",
    "\n",
    "        with open(filename, \"rb\") as file:\n",
    "            # Create a transcription of the audio file\n",
    "            transcription = self.grok_client.audio.transcriptions.create(\n",
    "              file=(filename, file.read()), # Required audio file\n",
    "              model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n",
    "              prompt=\"Specify context or spelling\",  # Optional\n",
    "              response_format=\"json\",  # Optional\n",
    "              # language=\"ar\",  # Optional\n",
    "              temperature=0.0  # Optional\n",
    "            )\n",
    "            # Print th\n",
    "        \n",
    "        return transcription.text\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        response_data = self.qa_pipeline.invoke(prompt)\n",
    "        print('Final prompt to LLM after RAG:\\n')\n",
    "        print(response_data['query'])\n",
    "        return response_data['result']\n",
    "\n",
    "    def text_to_speech(self, text, output_path=r'D:\\GitHub projects\\Mic_Server_Test\\Backend\\output_voices\\speech.mp3'):\n",
    "        start = time.time()\n",
    "        tts = gTTS(text, lang='ar', slow=False)\n",
    "        \n",
    "        tts.save(output_path)\n",
    "        print(f'Text to sound time: {time.time() - start}')\n",
    "\n",
    "        # output_path = os.path.join(os.getcwd(), output_path)\n",
    "        \n",
    "        # music = pyglet.media.load(\"D:\\GitHub projects\\Mic_Server_Test\\Backend\\output_voices\\speech.mp3\", streaming=False)\n",
    "        # music.play()\n",
    "        # os.remove(output_path)\n",
    "        return tts\n",
    "                \n",
    "    def process_audio_file(self, audio_path):\n",
    "        start = time.time()\n",
    "        transcription = self.speach_to_text(audio_path)\n",
    "        print(f'Sound to text time: {time.time() - start}')\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        arabic_instruction = \"يرجى تقديم الإجابة باللغة العربية فقط  واجعل الأجابة مختصرة في سطر قدر الامكان الامكان: \"\n",
    "        # arabic_instruction = \"response in just 2 lines: \"\n",
    "        prompt = f\"{arabic_instruction}\\n{transcription}\"\n",
    "        print(prompt)\n",
    "        response = self.generate_response(prompt)\n",
    "        print(f'Model response time: {time.time() - start}')\n",
    "        \n",
    "        \n",
    "        tts_output = self.text_to_speech(response)\n",
    "        return transcription, response, tts_output\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audio_path = r\"C:\\Users\\zmlka\\Documents\\Sound Recordings\\Recording (3).m4a\"\n",
    "\n",
    "    voice_bot = RAGVoiceBot(\n",
    "        knowldge_path='./knowledge_base',\n",
    "        groq_token_path='groq_token.txt',\n",
    "        vector_db_path='vector_db'\n",
    "    )\n",
    "\n",
    "    transcription, response = voice_bot.process_audio_file(audio_path)\n",
    "\n",
    "    print('='*50)\n",
    "\n",
    "    print(\"USER:\", transcription)\n",
    "    print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0a6ac-bd9c-4eab-9fed-0c4143fe294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = Groq()\n",
    "\n",
    "# Specify the path to the audio file\n",
    "filename = r\"C:\\Users\\zmlka\\Documents\\Sound Recordings\\Recording (3).m4a\" # Replace with your audio file!\n",
    "\n",
    "# Open the audio file\n",
    "with open(filename, \"rb\") as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      file=(filename, file.read()), # Required audio file\n",
    "      model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n",
    "      prompt=\"Specify context or spelling\",  # Optional\n",
    "      response_format=\"json\",  # Optional\n",
    "      # language=\"ar\",  # Optional\n",
    "      temperature=0.0  # Optional\n",
    "    )\n",
    "    # Print the transcription text\n",
    "    print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e9e20-17f1-4b3b-9c98-c57887144f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from faster_whisper import WhisperModel\n",
    "from gtts import gTTS\n",
    "from groq import Groq\n",
    "from playsound import playsound \n",
    "import pyglet\n",
    "import io\n",
    "from elevenlabs import save, play\n",
    "from elevenlabs.client import ElevenLabs\n",
    "\n",
    "\n",
    "class RAGVoiceBot:\n",
    "    def __init__(self, vector_db_path, knowldge_path, groq_token_path, whisper_size='base', model_name='llama-3.1-70b-versatile'):\n",
    "\n",
    "        self.load_groq_token(groq_token_path)\n",
    "        self.grok_client = Groq()\n",
    "\n",
    "        self.elevenlabs_client = ElevenLabs(api_key='sk_4b8af34b2615328298ba8718fc90797eafbcc39d08382917')\n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        self.whisper_model = self.initialize_STT_model(whisper_size)\n",
    "        self.llm = self.initialize_llm(model_name)\n",
    "        self.vector_db = self.intialize_vectore_db(vector_db_path, knowldge_path)\n",
    "        # self.qa_pipeline = self.initialize_qa_pipeline(vector_db_path, knowldge_path, k=3)\n",
    "\n",
    "    def load_groq_token(self, groq_token_path):\n",
    "        with open(groq_token_path, 'r') as f:\n",
    "            os.environ[\"GROQ_API_KEY\"] = f.readline().strip()\n",
    "\n",
    "    def initialize_STT_model(self, whisper_size):\n",
    "        num_cores = os.cpu_count() // 2\n",
    "        return WhisperModel(\n",
    "            whisper_size,\n",
    "            device=\"cpu\",\n",
    "            # compute_type=\"int8\",\n",
    "            cpu_threads=num_cores\n",
    "        )\n",
    "        \n",
    "\n",
    "    def initialize_llm(self, model_name):\n",
    "        return ChatGroq(\n",
    "            model=model_name\n",
    "        )\n",
    "\n",
    "    def intialize_vectore_db(self, vector_db_path, knowldge_path):\n",
    "        if os.path.exists(vector_db_path):\n",
    "            print('Loading exisiting vector database')\n",
    "            vec_db = FAISS.load_local(vector_db_path, self.embedding_model, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print('Creating new vector database')\n",
    "            vec_db = self.create_vector_database_texts(knowldge_path)\n",
    "            vec_db.save_local(vector_db_path)\n",
    "\n",
    "        return vec_db\n",
    "    \n",
    "    def initialize_qa_pipeline(self, vector_db_path, knowldge_path, k):\n",
    "        if os.path.exists(vector_db_path):\n",
    "            print('Loading exisiting vector database')\n",
    "            vec_db = FAISS.load_local(vector_db_path, self.embedding_model, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print('Creating new vector database')\n",
    "            vec_db = self.create_vector_database_texts(knowldge_path)\n",
    "            vec_db.save_local(vector_db_path)\n",
    "\n",
    "        # The RAG pipeline\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=vec_db.as_retriever(search_kwargs={'k': 5}),\n",
    "            # return_source_documents=True\n",
    "        )\n",
    "\n",
    "    \n",
    "    def get_relevant_chunks(self, query, k=5):\n",
    "        docs = self.vector_db.similarity_search(query, k=k)\n",
    "        return [doc.page_content for doc in docs]\n",
    "\n",
    "    def construct_prompt(self, chunks, query):\n",
    "        context = \"\\n\".join([f\"- {chunk}\" for chunk in chunks])\n",
    "    \n",
    "        rag_template = f\"\"\"You are given a user query, some textual context and rules, all inside xml tags. You have to answer the query based on the context while respecting the rules.\n",
    "\n",
    "<context>\n",
    " {context}\n",
    "</context>\n",
    "\n",
    "<rules>\n",
    "- If you don't know, just say so.\n",
    "- If you are not sure, ask for clarification.\n",
    "- Answer in the same language as the user query.\n",
    "- If the context appears unreadable or of poor quality, tell the user then answer as best as you can.\n",
    "- If the answer is not in the context but you think you know the answer, explain that to the user then answer with your own knowledge.\n",
    "- Answer directly and without using xml tags.\n",
    "</rules>\n",
    "\n",
    "<user_query>\n",
    "{query}\n",
    "</user_query>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        return rag_template\n",
    "\n",
    "    def get_model_response(self, full_prompt, model_name='llama-3.1-70b-versatile'):\n",
    "        response = self.grok_client.chat.completions.create(\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a professional conference assistant fluent in Arabic and English. Respond concisely and professionally ONLY IN ARABIC\"},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ],\n",
    "            max_tokens=250,\n",
    "            temperature=0.4,\n",
    "        )\n",
    "    \n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "    def process_user_message(self, query):\n",
    "        relevant_chunks = self.get_relevant_chunks(query)\n",
    "        full_prompt_after_rag = self.construct_prompt(relevant_chunks, query)\n",
    "        response = self.get_model_response(full_prompt_after_rag, model_name='llama-3.1-70b-versatile')\n",
    "    \n",
    "        return response\n",
    "    \n",
    "    def create_vector_database_docs_pdfs(self, knowldge_path):\n",
    "        loader = PyPDFLoader(knowldge_path)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        return FAISS.from_documents(docs, self.embedding_model)\n",
    "    \n",
    "    def create_vector_database_texts(self, knowldge_path):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "        all_chunks = []\n",
    "        \n",
    "        for filename in os.listdir(knowldge_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(knowldge_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text_content = file.read()\n",
    "                    \n",
    "                    chunks = text_splitter.split_text(text_content)\n",
    "                    all_chunks.extend(chunks)\n",
    "                    \n",
    "        return FAISS.from_texts(all_chunks, self.embedding_model)\n",
    "    \n",
    "    def speach_to_text(self, wav_audio):\n",
    "        # print('we now opened the speech_to_text method .........')\n",
    "        # # segments, _ = self.whisper_model.transcribe(audio_path, language='ar')\n",
    "        # # return ''.join(segment.text for segment in segments)\n",
    "        # # with open(audio_path, \"rb\") as file:\n",
    "        # # audio_file \n",
    "        # audio_file = (\"audio.wav\", wav_audio)\n",
    "        # transcription = self.grok_client.audio.transcriptions.create(\n",
    "        #     file=audio_file, \n",
    "        #     model=\"whisper-large-v3-turbo\",\n",
    "        #     prompt=\"Specify context or spelling\",\n",
    "        #     response_format=\"json\",\n",
    "        #     # language='ar',\n",
    "        #     temperature=0.0,\n",
    "        # )\n",
    "\n",
    "        with open(wav_audio, \"rb\") as file:\n",
    "            # Create a transcription of the audio file\n",
    "            transcription = self.grok_client.audio.transcriptions.create(\n",
    "              file=(filename, file.read()), # Required audio file\n",
    "              model=\"whisper-large-v3\", # Required model to use for transcription\n",
    "              prompt=\"Specify context or spelling\",  # Optional\n",
    "              response_format=\"json\",  # Optional\n",
    "              # language=\"ar\",  # Optional\n",
    "              temperature=0.0  # Optional\n",
    "            )\n",
    "            # Print th\n",
    "        \n",
    "        return transcription.text.strip()\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        response_data = self.qa_pipeline.invoke(prompt)\n",
    "        print('Final prompt to LLM after RAG:\\n')\n",
    "        print(response_data['query'])\n",
    "        return response_data['result']\n",
    "\n",
    "    def text_to_speech(self, text, output_path=r'D:\\GitHub projects\\Mic_Server_Test\\Backend\\output_voices\\speech.mp3'):\n",
    "        start = time.time()\n",
    "        tts = gTTS(text, lang='ar', slow=False)\n",
    "        \n",
    "        tts.save(output_path)\n",
    "        print(f'Text to sound time: {time.time() - start}')\n",
    "\n",
    "        # output_path = os.path.join(os.getcwd(), output_path)\n",
    "        \n",
    "        # music = pyglet.media.load(\"D:\\GitHub projects\\Mic_Server_Test\\Backend\\output_voices\\speech.mp3\", streaming=False)\n",
    "        # music.play()\n",
    "        # os.remove(output_path)\n",
    "        return tts\n",
    "\n",
    "    def text_to_sound(self, text):\n",
    "        audio = self.elevenlabs_client.generate(\n",
    "            text=text,\n",
    "            voice=\"IK7YYZcSpmlkjKrQxbSn\",\n",
    "            model=\"eleven_multilingual_v2\"\n",
    "        )\n",
    "        \n",
    "        # save(audio, \"output.mp3\")\n",
    "        play(audio)\n",
    "        \n",
    "    def process_audio_file(self, audio_path):\n",
    "        # start = time.time()\n",
    "        # transcription = self.speach_to_text(audio_path)\n",
    "        # print(f'Sound to text time: {time.time() - start}')\n",
    "\n",
    "        # start = time.time()\n",
    "        \n",
    "        # arabic_instruction = \"يرجى تقديم الإجابة باللغة العربية فقط  واجعل الأجابة مختصرة في سطر قدر الامكان الامكان: \"\n",
    "        # # arabic_instruction = \"response in just 2 lines: \"\n",
    "        # prompt = f\"{arabic_instruction}\\n{transcription}\"\n",
    "        # print(prompt)\n",
    "        # response = self.generate_response(prompt)\n",
    "        # print(f'Model response time: {time.time() - start}')\n",
    "        \n",
    "        \n",
    "        # tts_output = self.text_to_speech(response)\n",
    "        # return transcription, response, tts_output\n",
    "\n",
    "        start = time.time()\n",
    "        transcription = self.speach_to_text(audio_path)\n",
    "        print(f'Sound to text time: {time.time() - start}')\n",
    "        print(f'Transcritption: {transcription}')\n",
    "        print('='*50)\n",
    "        \n",
    "        start = time.time()\n",
    "        response = self.process_user_message(transcription)\n",
    "        \n",
    "        print(f'Model response: {response}')\n",
    "        print(f'Model response time: {time.time() - start}')\n",
    "        print('='*50)\n",
    "        \n",
    "        tts_output = self.text_to_sound(response)\n",
    "        return transcription, response, tts_output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audio_path = r\"C:\\Users\\zmlka\\Documents\\Sound Recordings\\Recording (3).m4a\"\n",
    "\n",
    "    print('Initializing all things')\n",
    "    voice_bot = RAGVoiceBot(\n",
    "        knowldge_path='./knowledge_base',\n",
    "        groq_token_path='groq_token.txt',\n",
    "        vector_db_path='vector_db'\n",
    "    )\n",
    "\n",
    "    print('='*50)\n",
    "    transcription, response, tts_output = voice_bot.process_audio_file(audio_path)\n",
    "\n",
    "    print('='*50)\n",
    "\n",
    "    # print(\"USER:\", transcription)\n",
    "    # print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f55f2-9a2b-41d7-a8e6-d445d5443363",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = r\"C:\\Users\\zmlka\\Documents\\Sound Recordings\\Recording (16).m4a\"\n",
    "transcription, response, tts_output = voice_bot.process_audio_file(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3523025b-e6d8-4fc9-8164-eec67dcb3441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
