{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ac533e-39b8-466e-bef6-d425d3a8831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "def generate_embeddings():\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L12-v2')\n",
    "    return embedding_model\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    reader = PdfReader(pdf_file)\n",
    "    raw_text = \"\"\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            raw_text += text\n",
    "    return raw_text\n",
    "    \n",
    "def create_vector_database_texts(raw_text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024, chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    texts = text_splitter.split_text(raw_text)\n",
    "    vec_db = FAISS.from_texts(texts, generate_embeddings())\n",
    "    return vec_db\n",
    "\n",
    "def create_vector_database_docs(url):\n",
    "    loader = PyPDFLoader(url)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    vec_db = FAISS.from_documents(docs, generate_embeddings())\n",
    "    return vec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6fe3dc-683b-41c1-aceb-9c59e319db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a13f8fb-b3aa-4be4-8c35-e7dc957f0e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zmlka\\AppData\\Local\\Temp\\ipykernel_18708\\2396280315.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L12-v2')\n",
      "C:\\Users\\zmlka\\anaconda3\\envs\\torch\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\zmlka\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "db = create_vector_database_docs('Mohamed Hassan.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8febac-9500-4aa4-ac52-cb8795558bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('groq_token.txt', 'r') as f:\n",
    "    groq_token = f.readline()\n",
    "    \n",
    "os.environ[\"GROQ_API_KEY\"] = groq_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c56d79f-58f9-401f-833d-08ebde1ad9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "559f3895-9479-4ebc-9535-afd5fa4e024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "جازان هي منطقة ساحلية في جنوب غرب المملكة العربية السعودية، تشتهر بالمناظر الطبيعية الخلابة والجزر الجميلة. بعض من أجمل المناطق السياحية في جازان تشمل:\n",
      "\n",
      "1. جزر فرسان: مجموعة من الجزر الجميلة التي تقع قبالة الساحل الغربي لجازان. تشتهر الجزر بالشواطئ الرملية البيضاء والمياه الصافية والغوص.\n",
      "2. وادي لجب: وادي جميل يقع في vùng جازان، يشتهر بالطبيعة الخلابة والمناظر الجبلية.\n",
      "3. خوربارك: شاطئ رملية جميل يقع في مدينة جازان، يعرف بشواطئه الرملية البيضاء والمياه الصافية.\n",
      "4. منتزه جبل دودا: منتزه جميل يقع على قمة جبل دودا، يعرف بالمناظر الخلابة والهواء النقي.\n",
      "5. سوق جازان القديم: سوق تقليدي يقع في مدينة جازان، يعرف بالسلع التقليدية والمأكولات المحلية.\n",
      "\n",
      "هذه بعض من المناطق الجميلة في جازان، هناك الكثير من الأماكن الأخرى التي يمكنك زيارتها أيضاً.\n"
     ]
    }
   ],
   "source": [
    "response = qa_pipeline.invoke(\"اخبرني عن مناطق جميلة بجازان السعودية\")\n",
    "resutl = response['result']\n",
    "# print(response)\n",
    "print('='*50)\n",
    "print(resutl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "505dcfa0-911e-4d86-9b76-553d0026046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound to text time: 1.2131483554840088\n",
      "USER:  السلام عليكم كيف الحالة\n",
      "Model response time: 1.6860651969909668\n",
      "Assistant:  وعليكم السلام ورحمة الله وبركاته. الحالة جيدة، شكراً. كيف يمكنني مساعدتك؟\n",
      "text to sound time: 0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from faster_whisper import WhisperModel\n",
    "from gtts import gTTS\n",
    "from groq import Groq\n",
    "import time\n",
    "\n",
    "# Set up the model configuration\n",
    "whisper_size = 'base'  # Model size; options include 'tiny', 'small', 'medium', etc.\n",
    "num_cores = os.cpu_count()  # Use half of available CPU cores if needed\n",
    "\n",
    "# Initialize the Whisper model\n",
    "whisper_model = WhisperModel(\n",
    "    whisper_size,\n",
    "    device=\"cpu\",        # Use \"cuda\" for GPU, \"cpu\" for CPU\n",
    "    compute_type=\"int8\",   # Change to \"float16\" or \"int8\" as per your GPU capability\n",
    "    cpu_threads=num_cores // 2  # Adjust based on your requirements\n",
    ")\n",
    "\n",
    "def groq_prompt(prompt, model='llama-3.1-70b-versatile'):\n",
    "    grok_client = Groq(\n",
    "        api_key='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'\n",
    "    )\n",
    "    chat_completion = grok_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt\n",
    "            }\n",
    "        ],\n",
    "        model=model\n",
    "    )\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response\n",
    "    \n",
    "def wav_to_text(audio_path):\n",
    "    segments, _ = whisper_model.transcribe(audio_path, language='ar')\n",
    "    text = ''.join(segment.text for segment in segments)\n",
    "    return text\n",
    "\n",
    "# Example: Transcribe an audio file\n",
    "audio_path = r\"C:\\Users\\zmlka\\Documents\\Sound Recordings\\Recording (3).m4a\"  # Replace with the path to your audio file\n",
    "start = time.time()\n",
    "transcription = wav_to_text(audio_path)\n",
    "print(f'Sound to text time: {time.time() - start}')\n",
    "\n",
    "print(\"USER:\", transcription)\n",
    "start = time.time()\n",
    "# response = groq_prompt(transcription)\n",
    "response = qa_pipeline.invoke(transcription)['result']\n",
    "print(f'Model response time: {time.time() - start}')\n",
    "print('Assistant: ', response)\n",
    "\n",
    "start = time.time()\n",
    "tts = gTTS(response, lang='ar')\n",
    "print(f'text to sound time: {time.time() - start}')\n",
    "tts.save('speech.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc77e56-7568-4ce2-8c50-f7241777e272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zmlka\\AppData\\Local\\Temp\\ipykernel_18176\\1688035807.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L12-v2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading exisiting vector database\n",
      "Sound to text time: 1.276045560836792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zmlka\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response time: 3.2341606616973877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 259 for command:\n",
      "        play speech.mp3 wait\n",
      "    The driver cannot recognize the specified command parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to sound time: 1.4821882247924805\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "\n    Error 259 for command:\n        play speech.mp3 wait\n    The driver cannot recognize the specified command parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPlaysoundException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m\n\u001b[0;32m    101\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mzmlka\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSound Recordings\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRecording (3).m4a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m voice_bot \u001b[38;5;241m=\u001b[39m RAGVoiceBot(\n\u001b[0;32m    103\u001b[0m     knowldge_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMohamed Hassan.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    104\u001b[0m     groq_token_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroq_token.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    105\u001b[0m     vector_db_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_db\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    106\u001b[0m )\n\u001b[1;32m--> 107\u001b[0m transcription, response \u001b[38;5;241m=\u001b[39m voice_bot\u001b[38;5;241m.\u001b[39mprocess_audio_file(audio_path)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSER:\u001b[39m\u001b[38;5;124m\"\u001b[39m, transcription)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m, in \u001b[0;36mRAGVoiceBot.process_audio_file\u001b[1;34m(self, audio_path)\u001b[0m\n\u001b[0;32m     91\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_response(transcription)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel response time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_to_speech(response)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transcription, response\n",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m, in \u001b[0;36mRAGVoiceBot.text_to_speech\u001b[1;34m(self, text, output_path)\u001b[0m\n\u001b[0;32m     80\u001b[0m tts\u001b[38;5;241m.\u001b[39msave(output_path)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText to sound time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m playsound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     83\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\playsound.py:73\u001b[0m, in \u001b[0;36m_playsoundWin\u001b[1;34m(sound, block)\u001b[0m\n\u001b[0;32m     71\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m     winCommand(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sound))\n\u001b[1;32m---> 73\u001b[0m     winCommand(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplay \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sound, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m wait\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     74\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\playsound.py:64\u001b[0m, in \u001b[0;36m_playsoundWin.<locals>.winCommand\u001b[1;34m(*command)\u001b[0m\n\u001b[0;32m     60\u001b[0m     exceptionMessage \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Error \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(errorCode) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for command:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m command\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     62\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m errorBuffer\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     63\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(exceptionMessage)\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PlaysoundException(exceptionMessage)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mPlaysoundException\u001b[0m: \n    Error 259 for command:\n        play speech.mp3 wait\n    The driver cannot recognize the specified command parameter."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from faster_whisper import WhisperModel\n",
    "from gtts import gTTS\n",
    "from groq import Groq\n",
    "from playsound import playsound \n",
    "\n",
    "class RAGVoiceBot:\n",
    "    def __init__(self, vector_db_path, knowldge_path, groq_token_path, whisper_size='base', model_name='llama-3.1-70b-versatile'):\n",
    "\n",
    "        self.load_groq_token(groq_token_path)\n",
    "        \n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L12-v2')\n",
    "        self.whisper_model = self.initialize_STT_model(whisper_size)\n",
    "        self.llm = self.initialize_llm(model_name)\n",
    "        self.qa_pipeline = self.initialize_qa_pipeline(vector_db_path, knowldge_path)\n",
    "\n",
    "    \n",
    "    def load_groq_token(self, groq_token_path):\n",
    "        with open(groq_token_path, 'r') as f:\n",
    "            os.environ[\"GROQ_API_KEY\"] = f.readline().strip()\n",
    "\n",
    "    def initialize_STT_model(self, whisper_size):\n",
    "        num_cores = os.cpu_count() // 2\n",
    "        return WhisperModel(\n",
    "            whisper_size,\n",
    "            device=\"cpu\",\n",
    "            compute_type=\"int8\",\n",
    "            cpu_threads=num_cores\n",
    "        )\n",
    "\n",
    "    def initialize_llm(self, model_name):\n",
    "        return ChatGroq(\n",
    "            model=model_name\n",
    "        )\n",
    "\n",
    "    def initialize_qa_pipeline(self, vector_db_path, knowldge_path):\n",
    "        if os.path.exists(vector_db_path):\n",
    "            print('Loading exisiting vector database')\n",
    "            vec_db = FAISS.load_local(vector_db_path, self.embedding_model, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print('Creating new vector database')\n",
    "            vec_db = self.create_vector_database_docs(knowldge_path)\n",
    "            vec_db.save_local(vector_db_path)\n",
    "\n",
    "\n",
    "        # The RAG pipeline\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=vec_db.as_retriever(),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "    def create_vector_database_docs(self, knowldge_path):\n",
    "        loader = PyPDFLoader(knowldge_path)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        return FAISS.from_documents(docs, self.embedding_model)\n",
    "        \n",
    "\n",
    "    def speach_to_text(self, audio_path):\n",
    "        segments, _ = self.whisper_model.transcribe(audio_path)\n",
    "        return ''.join(segment.text for segment in segments)\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        return self.qa_pipeline.invoke(prompt)['result']\n",
    "\n",
    "    def text_to_speech(self, text, output_path='speech.mp3'):\n",
    "        start = time.time()\n",
    "        tts = gTTS(text, lang='ar')\n",
    "        tts.save(output_path)\n",
    "        print(f'Text to sound time: {time.time() - start}')\n",
    "        playsound('speech.mp3')\n",
    "        os.remove('speech.mp3')\n",
    "        \n",
    "    def process_audio_file(self, audio_path):\n",
    "        start = time.time()\n",
    "        transcription = self.speach_to_text(audio_path)\n",
    "        print(f'Sound to text time: {time.time() - start}')\n",
    "\n",
    "        start = time.time()\n",
    "        response = self.generate_response(transcription)\n",
    "        print(f'Model response time: {time.time() - start}')\n",
    "        \n",
    "        \n",
    "        self.text_to_speech(response)\n",
    "\n",
    "        return transcription, response\n",
    "\n",
    "\n",
    "\n",
    "audio_path = r\"C:\\Users\\zmlka\\Documents\\Sound Recordings\\Recording (3).m4a\"\n",
    "voice_bot = RAGVoiceBot(\n",
    "    knowldge_path='Mohamed Hassan.pdf',\n",
    "    groq_token_path='groq_token.txt',\n",
    "    vector_db_path='vector_db'\n",
    ")\n",
    "transcription, response = voice_bot.process_audio_file(audio_path)\n",
    "print(\"USER:\", transcription)\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c329b9c-dcf8-41c3-9216-89d926e7b80c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
