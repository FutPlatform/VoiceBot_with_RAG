{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c737396e-a94c-4f5a-923b-fe281e06f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zmlka\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we now opened the speech_to_text method .........\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'file must be one of the following types: [flac mp3 mp4 mpeg mpga m4a ogg opus wav webm]', 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 154\u001b[0m\n\u001b[0;32m    146\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mzmlka\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSound Recordings\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRecording (3).m4a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m voice_bot \u001b[38;5;241m=\u001b[39m RAGVoiceBot(\n\u001b[0;32m    149\u001b[0m     knowldge_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./knowledge_base\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    150\u001b[0m     groq_token_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroq_token.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    151\u001b[0m     vector_db_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_db\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    152\u001b[0m )\n\u001b[1;32m--> 154\u001b[0m transcription, response \u001b[38;5;241m=\u001b[39m voice_bot\u001b[38;5;241m.\u001b[39mprocess_audio_file(audio_path)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSER:\u001b[39m\u001b[38;5;124m\"\u001b[39m, transcription)\n",
      "Cell \u001b[1;32mIn[2], line 127\u001b[0m, in \u001b[0;36mRAGVoiceBot.process_audio_file\u001b[1;34m(self, audio_path)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_audio_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_path):\n\u001b[0;32m    126\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 127\u001b[0m     transcription \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeach_to_text(audio_path)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSound to text time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    130\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[2], line 94\u001b[0m, in \u001b[0;36mRAGVoiceBot.speach_to_text\u001b[1;34m(self, wav_audio)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# segments, _ = self.whisper_model.transcribe(audio_path, language='ar')\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# return ''.join(segment.text for segment in segments)\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# with open(audio_path, \"rb\") as file:\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# audio_file \u001b[39;00m\n\u001b[0;32m     93\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, wav_audio)\n\u001b[1;32m---> 94\u001b[0m transcription2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mtranscriptions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     95\u001b[0m     file\u001b[38;5;241m=\u001b[39maudio_file, \n\u001b[0;32m     96\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhisper-large-v3-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     97\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecify context or spelling\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     response_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# language='ar',\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    101\u001b[0m )\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transcription2\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\groq\\resources\\audio\\transcriptions.py:120\u001b[0m, in \u001b[0;36mTranscriptions.create\u001b[1;34m(self, file, model, language, prompt, response_format, temperature, timestamp_granularities, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m files:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# It should be noted that the actual Content-Type header that will be\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# sent to the server will contain a `boundary` parameter, e.g.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# multipart/form-data; boundary=---abc--\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultipart/form-data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    119\u001b[0m     TranscriptionCreateResponse,\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/audio/transcriptions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    122\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(body, transcription_create_params\u001b[38;5;241m.\u001b[39mTranscriptionCreateParams),\n\u001b[0;32m    123\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    124\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    125\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    126\u001b[0m         ),\n\u001b[0;32m    127\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast(\n\u001b[0;32m    128\u001b[0m             Any, TranscriptionCreateResponse\n\u001b[0;32m    129\u001b[0m         ),  \u001b[38;5;66;03m# Union types cannot be passed in as arguments in the type system\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     ),\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\groq\\_base_client.py:1225\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1213\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1220\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1222\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1223\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1224\u001b[0m     )\n\u001b[1;32m-> 1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\groq\\_base_client.py:920\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    913\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    919\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    921\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    922\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    923\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    924\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    925\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    926\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\groq\\_base_client.py:1018\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1015\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1017\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1018\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1021\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1026\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'file must be one of the following types: [flac mp3 mp4 mpeg mpga m4a ogg opus wav webm]', 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from faster_whisper import WhisperModel\n",
    "from gtts import gTTS\n",
    "from groq import Groq\n",
    "from playsound import playsound \n",
    "import pyglet\n",
    "import io\n",
    "\n",
    "class RAGVoiceBot:\n",
    "    def __init__(self, vector_db_path, knowldge_path, groq_token_path, whisper_size='base', model_name='llama-3.1-70b-versatile'):\n",
    "\n",
    "        self.load_groq_token(groq_token_path)\n",
    "        self.client = Groq()\n",
    "        \n",
    "        self.embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        self.whisper_model = self.initialize_STT_model(whisper_size)\n",
    "        self.llm = self.initialize_llm(model_name)\n",
    "        self.qa_pipeline = self.initialize_qa_pipeline(vector_db_path, knowldge_path, k=3)\n",
    "\n",
    "    def load_groq_token(self, groq_token_path):\n",
    "        with open(groq_token_path, 'r') as f:\n",
    "            os.environ[\"GROQ_API_KEY\"] = f.readline().strip()\n",
    "\n",
    "    def initialize_STT_model(self, whisper_size):\n",
    "        num_cores = os.cpu_count() // 2\n",
    "        return WhisperModel(\n",
    "            whisper_size,\n",
    "            device=\"cpu\",\n",
    "            # compute_type=\"int8\",\n",
    "            cpu_threads=num_cores\n",
    "        )\n",
    "        \n",
    "\n",
    "    def initialize_llm(self, model_name):\n",
    "        return ChatGroq(\n",
    "            model=model_name\n",
    "        )\n",
    "\n",
    "    def initialize_qa_pipeline(self, vector_db_path, knowldge_path, k):\n",
    "        if os.path.exists(vector_db_path):\n",
    "            print('Loading exisiting vector database')\n",
    "            vec_db = FAISS.load_local(vector_db_path, self.embedding_model, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print('Creating new vector database')\n",
    "            vec_db = self.create_vector_database_texts(knowldge_path)\n",
    "            vec_db.save_local(vector_db_path)\n",
    "\n",
    "            \n",
    "        # The RAG pipeline\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=vec_db.as_retriever(search_kwargs={'k': 5}),\n",
    "            # return_source_documents=True\n",
    "        )\n",
    "\n",
    "    def create_vector_database_docs_pdfs(self, knowldge_path):\n",
    "        loader = PyPDFLoader(knowldge_path)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        return FAISS.from_documents(docs, self.embedding_model)\n",
    "    \n",
    "    def create_vector_database_texts(self, knowldge_path):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "        all_chunks = []\n",
    "        \n",
    "        for filename in os.listdir(knowldge_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(knowldge_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text_content = file.read()\n",
    "                    \n",
    "                    chunks = text_splitter.split_text(text_content)\n",
    "                    all_chunks.extend(chunks)\n",
    "                    \n",
    "        return FAISS.from_texts(all_chunks, self.embedding_model)\n",
    "    \n",
    "    def speach_to_text(self, wav_audio):\n",
    "        print('we now opened the speech_to_text method .........')\n",
    "        # segments, _ = self.whisper_model.transcribe(audio_path, language='ar')\n",
    "        # return ''.join(segment.text for segment in segments)\n",
    "        # with open(audio_path, \"rb\") as file:\n",
    "        # audio_file \n",
    "        audio_file = (\"audio.wav\", wav_audio)\n",
    "        transcription2 = self.client.audio.transcriptions.create(\n",
    "            file=audio_file, \n",
    "            model=\"whisper-large-v3-turbo\",\n",
    "            prompt=\"Specify context or spelling\",\n",
    "            response_format=\"json\",\n",
    "            # language='ar',\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        \n",
    "        return transcription2.text\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        response_data = self.qa_pipeline.invoke(prompt)\n",
    "        print('Final prompt to LLM after RAG:\\n')\n",
    "        print(response_data['query'])\n",
    "        return response_data['result']\n",
    "\n",
    "    def text_to_speech(self, text, output_path=r'D:\\GitHub projects\\Mic_Server_Test\\Backend\\output_voices\\speech.mp3'):\n",
    "        start = time.time()\n",
    "        tts = gTTS(text, lang='ar', slow=False)\n",
    "        \n",
    "        tts.save(output_path)\n",
    "        print(f'Text to sound time: {time.time() - start}')\n",
    "\n",
    "        # output_path = os.path.join(os.getcwd(), output_path)\n",
    "        \n",
    "        # music = pyglet.media.load(\"D:\\GitHub projects\\Mic_Server_Test\\Backend\\output_voices\\speech.mp3\", streaming=False)\n",
    "        # music.play()\n",
    "        # os.remove(output_path)\n",
    "        return tts\n",
    "                \n",
    "    def process_audio_file(self, audio_path):\n",
    "        start = time.time()\n",
    "        transcription = self.speach_to_text(audio_path)\n",
    "        print(f'Sound to text time: {time.time() - start}')\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        arabic_instruction = \"يرجى تقديم الإجابة باللغة العربية فقط  واجعل الأجابة مختصرة في سطر قدر الامكان الامكان: \"\n",
    "        # arabic_instruction = \"response in just 2 lines: \"\n",
    "        prompt = f\"{arabic_instruction}\\n{transcription}\"\n",
    "        print(prompt)\n",
    "        response = self.generate_response(prompt)\n",
    "        print(f'Model response time: {time.time() - start}')\n",
    "        \n",
    "        \n",
    "        tts_output = self.text_to_speech(response)\n",
    "        return transcription, response, tts_output\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audio_path = r\"C:\\Users\\zmlka\\Documents\\Sound Recordings\\Recording (3).m4a\"\n",
    "\n",
    "    voice_bot = RAGVoiceBot(\n",
    "        knowldge_path='./knowledge_base',\n",
    "        groq_token_path='groq_token.txt',\n",
    "        vector_db_path='vector_db'\n",
    "    )\n",
    "\n",
    "    transcription, response = voice_bot.process_audio_file(audio_path)\n",
    "\n",
    "    print('='*50)\n",
    "\n",
    "    print(\"USER:\", transcription)\n",
    "    print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf0a6ac-bd9c-4eab-9fed-0c4143fe294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, how are you? ماذا تعرف عن محمد حسن? Hello\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = Groq()\n",
    "\n",
    "# Specify the path to the audio file\n",
    "filename = r\"C:\\Users\\zmlka\\Documents\\Sound Recordings\\Recording (8).m4a\" # Replace with your audio file!\n",
    "\n",
    "# Open the audio file\n",
    "with open(filename, \"rb\") as file:\n",
    "    # Create a transcription of the audio file\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "      file=(filename, file.read()), # Required audio file\n",
    "      model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n",
    "      prompt=\"Specify context or spelling\",  # Optional\n",
    "      response_format=\"json\",  # Optional\n",
    "      # language=\"ar\",  # Optional\n",
    "      temperature=0.0  # Optional\n",
    "    )\n",
    "    # Print the transcription text\n",
    "    print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e07ce-80f7-41ce-a58d-5f51a05dc892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
